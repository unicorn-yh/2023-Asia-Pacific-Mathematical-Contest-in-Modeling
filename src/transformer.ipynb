{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import json\n",
    "from datetime import datetime\n",
    "from torch.utils.data import  DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 185, 270, 3)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "def image_to_patches(image, patch_size):\n",
    "    # 将图像转换为PyTorch张量\n",
    "    image = np.array(image)\n",
    "    image = torch.tensor(image).permute(2, 0, 1)  # 转换为 (C, H, W)\n",
    "    # 获取图像尺寸\n",
    "    C, H, W = image.shape\n",
    "    # 计算水平和垂直方向上的分块数量\n",
    "    num_patches_horizontal = W // patch_size\n",
    "    num_patches_vertical = H // patch_size\n",
    "    # 分割图像\n",
    "    patches = image.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)\n",
    "    patches = patches.contiguous().view(C, -1, patch_size, patch_size)\n",
    "    patches = patches.permute(1, 0, 2, 3)  # 转换为 (N, C, patch_size, patch_size)\n",
    "    return patches\n",
    "\n",
    "def flatten_patches(patches):\n",
    "    # 展平每个块\n",
    "    flat_patches = patches.reshape(patches.size(0), -1)\n",
    "    return flat_patches\n",
    "\n",
    "def calculate_number_of_patches(image_size, patch_size):\n",
    "    # 图像尺寸：(高度, 宽度)\n",
    "    height, width = image_size[0], image_size[1]\n",
    "    # 计算水平和垂直方向上的分块数量\n",
    "    num_patches_horizontal = width // patch_size\n",
    "    num_patches_vertical = height // patch_size\n",
    "    # 总的块数\n",
    "    total_patches = num_patches_horizontal * num_patches_vertical\n",
    "    return total_patches\n",
    "\n",
    "\n",
    "\n",
    "patch_size = 32\n",
    "image_count = 0\n",
    "data_x, data_y = [], []\n",
    "\n",
    "image_path = \"../Attachment/Attachment 1/\"\n",
    "for path in os.listdir(image_path):\n",
    "    image_count += 1\n",
    "    image = Image.open(image_path + path)\n",
    "    image = list(np.array(image))\n",
    "    #patches = image_to_patches(image, patch_size)\n",
    "    #flat_patches = flatten_patches(patches) # 展平分割后的块\n",
    "    #print(np.array(image).shape) # (185, 270, 3)\n",
    "    #print(patches.shape) # torch.Size([40, 3, 32, 32])\n",
    "    #print(flat_patches.shape) # torch.Size([40, 3072])\n",
    "    data_x.append(image)\n",
    "\n",
    "annonation_path = \"../Attachment/Attachment 1-Annotation/\"\n",
    "for path in os.listdir(annonation_path):\n",
    "    try:\n",
    "        num_of_apples = 0\n",
    "        f = open(annonation_path + path)\n",
    "        data = json.load(f)\n",
    "        for i in data['shapes']:\n",
    "            num_of_apples += 1\n",
    "        data_y.append(num_of_apples)\n",
    "    except:\n",
    "        data_y.append(0)\n",
    "\n",
    "data_x = np.array(data_x)\n",
    "data_y = np.array(data_y)\n",
    "print(data_x.shape)\n",
    "print(data_y.shape)\n",
    "\n",
    "# 计算块的数量\n",
    "#number_of_patches = flat_patches.shape[0] * image_count\n",
    "#print(number_of_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Einsum does matrix multiplication for query*keys for each training example\n",
    "        # with every other training example, don't be confused by einsum\n",
    "        # it's just a way to do batch matrix multiplication\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, embed_size, ff_hidden_size):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_size, ff_hidden_size)\n",
    "        self.fc2 = nn.Linear(ff_hidden_size, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_size, heads, ff_hidden_size, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.ff = FeedForwardNetwork(embed_size, ff_hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attention = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(attention + x)\n",
    "        forward = self.ff(x)\n",
    "        out = self.norm2(forward + x)\n",
    "        return self.dropout(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "def create_no_mask(seq_length):\n",
    "    return torch.zeros((seq_length, seq_length), dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountingHead(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(CountingHead, self).__init__()\n",
    "        self.fc = nn.Linear(embed_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.mean(dim=1)\n",
    "        out = self.fc(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, embed_size, num_layers, heads, ff_hidden_size, dropout, max_length):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_length, embed_size))\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderLayer(embed_size, heads, ff_hidden_size, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        #self.fc_out = nn.Linear(embed_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embed_size = embed_size\n",
    "         \n",
    "        # 添加任务特定的头部\n",
    "        self.counting_head = CountingHead(embed_size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, H, W, C = x.shape  # N: 批次大小, H: 高度, W: 宽度, C: 通道数\n",
    "        #positions = torch.arange(0, seq_length).expand(N, seq_length).to(device)\n",
    "        seq_length = H * W * C  # 序列长度\n",
    "        if seq_length > self.positional_encoding.size(1):\n",
    "            # 如果需要，扩展位置编码\n",
    "            self.positional_encoding = nn.Parameter(torch.zeros(1, seq_length, self.embed_size)).to(x.device)\n",
    "\n",
    "        out = self.dropout(self.encoder(x) + self.positional_encoding[:, :seq_length, :])\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, mask)\n",
    "\n",
    "        #out = self.fc_out(out)\n",
    "        out = self.counting_head(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader_train, loss_function, optimizer, epoch, train_size):\n",
    "    '''\n",
    "    完成一个 epoch 的训练\n",
    "    '''\n",
    "    sum_true = 0\n",
    "    sum_loss = 0.0\n",
    "    max_valid_acc = 0\n",
    "    model.train()\n",
    "    index = 0\n",
    "    total_data = len(data_loader_train)\n",
    "    for data in data_loader_train:\n",
    "        # 可视化训练过程\n",
    "        index += 1\n",
    "        print('Training batch {}/{}'.format(index,total_data),end='\\r')\n",
    "        # 选取对应批次数据的输入和标签\n",
    "        batch_x, batch_y = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # 模型预测\n",
    "        mask = create_no_mask(train_size).to(device)\n",
    "        y_hat = model(batch_x, mask)\n",
    "        loss = loss_function(y_hat, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()   # 梯度清零\n",
    "        loss.backward()         # 计算梯度\n",
    "        optimizer.step()        # 更新参数\n",
    "\n",
    "        y_hat = torch.tensor([torch.argmax(_) for _ in y_hat]).to(device)\n",
    "        sum_true += torch.sum(y_hat == batch_y).float()\n",
    "        sum_loss += loss.item()\n",
    "\n",
    "    train_acc = sum_true / train_size\n",
    "    train_loss = sum_loss / train_size\n",
    "\n",
    "    ''' valid_acc = valid()\n",
    "    if valid_acc > max_valid_acc:\n",
    "        torch.save(model, \"checkpoint.pt\")'''\n",
    "\n",
    "    print(f\"epoch: {epoch}, train loss: {train_loss:.4f}, train accuracy: {train_acc*100:.2f}%, valid accuracy: {valid_acc*100:.2f}%, time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()) }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 1/10\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4910284800 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Project\\Mathematical-Contest-In-Modeling\\src\\transformer.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Mathematical-Contest-In-Modeling/src/transformer.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# 进行训练\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Mathematical-Contest-In-Modeling/src/transformer.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Project/Mathematical-Contest-In-Modeling/src/transformer.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     train(model, data_loader_train, loss_function, optimizer, epoch, train_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Mathematical-Contest-In-Modeling/src/transformer.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# 对测试集进行预测\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Mathematical-Contest-In-Modeling/src/transformer.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m#predict()\u001b[39;00m\n",
      "\u001b[1;32md:\\Project\\Mathematical-Contest-In-Modeling\\src\\transformer.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Mathematical-Contest-In-Modeling/src/transformer.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# 模型预测\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Mathematical-Contest-In-Modeling/src/transformer.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m mask \u001b[39m=\u001b[39m create_no_mask(train_size)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Project/Mathematical-Contest-In-Modeling/src/transformer.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m y_hat \u001b[39m=\u001b[39m model(batch_x, mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Mathematical-Contest-In-Modeling/src/transformer.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(y_hat, batch_y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Mathematical-Contest-In-Modeling/src/transformer.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()   \u001b[39m# 梯度清零\u001b[39;00m\n",
      "File \u001b[1;32md:\\Download\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Download\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32md:\\Project\\Mathematical-Contest-In-Modeling\\src\\transformer.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Mathematical-Contest-In-Modeling/src/transformer.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m seq_length \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_encoding\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Mathematical-Contest-In-Modeling/src/transformer.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# 如果需要，扩展位置编码\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Mathematical-Contest-In-Modeling/src/transformer.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_encoding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(torch\u001b[39m.\u001b[39mzeros(\u001b[39m1\u001b[39m, seq_length, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_size))\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Project/Mathematical-Contest-In-Modeling/src/transformer.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_encoding[:, :seq_length, :])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Mathematical-Contest-In-Modeling/src/transformer.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Project/Mathematical-Contest-In-Modeling/src/transformer.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     out \u001b[39m=\u001b[39m layer(out, mask)\n",
      "File \u001b[1;32md:\\Download\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Download\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Download\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32md:\\Download\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2228\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2230\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2231\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2233\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4910284800 bytes."
     ]
    }
   ],
   "source": [
    "vocab_size = 2000  #（如果按像素处理）或分割块的数量\n",
    "embed_size = 512\n",
    "num_layers = 4\n",
    "heads = 8\n",
    "ff_hidden_size = 2048\n",
    "dropout = 0.1\n",
    "max_length = vocab_size * 4\n",
    "lr = 5e-5\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "model = Transformer(vocab_size,embed_size,num_layers,heads,ff_hidden_size,dropout,max_length).to(device)\n",
    "loss_function = nn.CrossEntropyLoss()                                       # 设置损失函数\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)  # 设置优化器\n",
    "\n",
    "data_x = torch.tensor(np.array(data_x))       \n",
    "data_y = torch.tensor(np.array(data_y)).long()    \n",
    "dataset = TensorDataset(data_x, data_y)\n",
    "dataset_size = data_x.size(dim = 0)\n",
    "train_size = int(dataset_size * 0.8)\n",
    "test_size = int(dataset_size * 0.2)\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "data_loader_train = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#data_loader_valid = DataLoader(dataset=dataset.valid, batch_size=batch_size, shuffle=False)\n",
    "data_loader_test = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# 进行训练\n",
    "for epoch in range(epochs):\n",
    "    train(model, data_loader_train, loss_function, optimizer, epoch, train_size)\n",
    "\n",
    "# 对测试集进行预测\n",
    "#predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[252, 251, 246],\n",
       "          [253, 254, 248],\n",
       "          [250, 249, 245],\n",
       "          ...,\n",
       "          [ 55,  69,  43],\n",
       "          [ 61,  74,  44],\n",
       "          [ 78,  91,  63]],\n",
       "\n",
       "         [[252, 248, 245],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 251],\n",
       "          ...,\n",
       "          [ 42,  58,  29],\n",
       "          [ 42,  57,  24],\n",
       "          [ 54,  67,  37]],\n",
       "\n",
       "         [[252, 248, 249],\n",
       "          [255, 255, 253],\n",
       "          [253, 249, 246],\n",
       "          ...,\n",
       "          [ 43,  56,  26],\n",
       "          [ 41,  54,  26],\n",
       "          [ 51,  64,  38]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 63,  64,  68],\n",
       "          [ 53,  54,  59],\n",
       "          [ 57,  55,  66],\n",
       "          ...,\n",
       "          [ 24,  38,  21],\n",
       "          [ 24,  38,  21],\n",
       "          [ 40,  52,  38]],\n",
       "\n",
       "         [[ 65,  68,  73],\n",
       "          [ 65,  66,  71],\n",
       "          [ 70,  68,  79],\n",
       "          ...,\n",
       "          [ 25,  39,  22],\n",
       "          [ 24,  38,  21],\n",
       "          [ 35,  46,  30]],\n",
       "\n",
       "         [[ 88,  91,  98],\n",
       "          [ 80,  83,  92],\n",
       "          [ 94,  94, 104],\n",
       "          ...,\n",
       "          [ 40,  52,  38],\n",
       "          [ 39,  51,  37],\n",
       "          [ 54,  65,  51]]],\n",
       "\n",
       "\n",
       "        [[[252, 254, 251],\n",
       "          [254, 254, 255],\n",
       "          [249, 253, 252],\n",
       "          ...,\n",
       "          [119, 114,  74],\n",
       "          [114, 111,  66],\n",
       "          [133, 130,  87]],\n",
       "\n",
       "         [[249, 254, 250],\n",
       "          [254, 255, 255],\n",
       "          [253, 255, 254],\n",
       "          ...,\n",
       "          [111, 105,  73],\n",
       "          [125, 119,  83],\n",
       "          [142, 140, 102]],\n",
       "\n",
       "         [[250, 252, 249],\n",
       "          [250, 255, 251],\n",
       "          [241, 245, 244],\n",
       "          ...,\n",
       "          [114, 114, 102],\n",
       "          [154, 152, 139],\n",
       "          [185, 181, 169]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[153, 153,  91],\n",
       "          [146, 147,  79],\n",
       "          [118, 119,  59],\n",
       "          ...,\n",
       "          [ 97, 116,  52],\n",
       "          [104, 124,  55],\n",
       "          [114, 132,  72]],\n",
       "\n",
       "         [[134, 134,  64],\n",
       "          [135, 138,  61],\n",
       "          [136, 139,  70],\n",
       "          ...,\n",
       "          [ 90, 110,  38],\n",
       "          [103, 126,  46],\n",
       "          [100, 120,  51]],\n",
       "\n",
       "         [[157, 162,  95],\n",
       "          [137, 138,  68],\n",
       "          [143, 145,  80],\n",
       "          ...,\n",
       "          [109, 129,  60],\n",
       "          [109, 129,  58],\n",
       "          [114, 135,  68]]],\n",
       "\n",
       "\n",
       "        [[[ 50, 108,  47],\n",
       "          [ 32,  95,  25],\n",
       "          [ 29,  89,  25],\n",
       "          ...,\n",
       "          [ 72, 123,  28],\n",
       "          [ 69, 125,  26],\n",
       "          [ 85, 133,  47]],\n",
       "\n",
       "         [[ 28,  91,  21],\n",
       "          [ 15,  86,   6],\n",
       "          [ 13,  81,   4],\n",
       "          ...,\n",
       "          [ 41, 101,   3],\n",
       "          [ 45, 107,   0],\n",
       "          [ 63, 114,  22]],\n",
       "\n",
       "         [[ 37,  99,  26],\n",
       "          [ 15,  87,   3],\n",
       "          [ 14,  83,   3],\n",
       "          ...,\n",
       "          [ 28,  83,   0],\n",
       "          [ 31,  87,   0],\n",
       "          [ 57, 106,  24]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 37, 111,  22],\n",
       "          [ 16, 102,   1],\n",
       "          [ 21, 103,   1],\n",
       "          ...,\n",
       "          [ 45, 127,   0],\n",
       "          [ 48, 134,   1],\n",
       "          [ 70, 144,  29]],\n",
       "\n",
       "         [[ 43, 124,  22],\n",
       "          [ 28, 116,   4],\n",
       "          [ 25, 111,   4],\n",
       "          ...,\n",
       "          [ 51, 134,   2],\n",
       "          [ 53, 141,   2],\n",
       "          [ 67, 145,  25]],\n",
       "\n",
       "         [[ 71, 141,  52],\n",
       "          [ 50, 128,  26],\n",
       "          [ 40, 115,  20],\n",
       "          ...,\n",
       "          [ 64, 142,  23],\n",
       "          [ 68, 148,  23],\n",
       "          [ 88, 157,  50]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[252, 252, 252],\n",
       "          [253, 255, 254],\n",
       "          [251, 251, 251],\n",
       "          ...,\n",
       "          [246, 255, 254],\n",
       "          [255, 253, 254],\n",
       "          [246, 255, 254]],\n",
       "\n",
       "         [[253, 248, 255],\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          ...,\n",
       "          [252, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [245, 253, 255]],\n",
       "\n",
       "         [[246, 243, 250],\n",
       "          [253, 252, 255],\n",
       "          [251, 251, 251],\n",
       "          ...,\n",
       "          [241, 251, 253],\n",
       "          [250, 255, 255],\n",
       "          [241, 251, 253]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 99, 129,  29],\n",
       "          [101, 136,  18],\n",
       "          [111, 144,  37],\n",
       "          ...,\n",
       "          [ 55, 152,  81],\n",
       "          [ 45, 145,  72],\n",
       "          [ 57, 149,  82]],\n",
       "\n",
       "         [[118, 151,  46],\n",
       "          [118, 156,  35],\n",
       "          [110, 144,  32],\n",
       "          ...,\n",
       "          [ 29, 141,  69],\n",
       "          [ 27, 144,  63],\n",
       "          [ 38, 144,  72]],\n",
       "\n",
       "         [[155, 182,  89],\n",
       "          [145, 177,  70],\n",
       "          [131, 161,  61],\n",
       "          ...,\n",
       "          [ 43, 148,  81],\n",
       "          [ 44, 154,  83],\n",
       "          [ 59, 161,  96]]],\n",
       "\n",
       "\n",
       "        [[[152, 149,  68],\n",
       "          [120, 116,  19],\n",
       "          [135, 132,  51],\n",
       "          ...,\n",
       "          [242, 251, 248],\n",
       "          [254, 254, 254],\n",
       "          [248, 254, 254]],\n",
       "\n",
       "         [[146, 144,  59],\n",
       "          [119, 116,   9],\n",
       "          [133, 128,  46],\n",
       "          ...,\n",
       "          [255, 255, 255],\n",
       "          [255, 255, 255],\n",
       "          [252, 254, 253]],\n",
       "\n",
       "         [[140, 143,  66],\n",
       "          [117, 119,  22],\n",
       "          [145, 137,  65],\n",
       "          ...,\n",
       "          [244, 245, 240],\n",
       "          [252, 254, 253],\n",
       "          [247, 248, 243]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[132, 153,  88],\n",
       "          [115, 141,  52],\n",
       "          [109, 133,  47],\n",
       "          ...,\n",
       "          [197, 203, 131],\n",
       "          [227, 238, 159],\n",
       "          [224, 233, 176]],\n",
       "\n",
       "         [[132, 153,  86],\n",
       "          [121, 150,  58],\n",
       "          [ 97, 123,  32],\n",
       "          ...,\n",
       "          [144, 143,  76],\n",
       "          [183, 187, 113],\n",
       "          [190, 193, 136]],\n",
       "\n",
       "         [[184, 202, 142],\n",
       "          [133, 158,  75],\n",
       "          [ 79, 102,  22],\n",
       "          ...,\n",
       "          [104,  99,  43],\n",
       "          [121, 121,  57],\n",
       "          [146, 148, 101]]],\n",
       "\n",
       "\n",
       "        [[[184, 193, 172],\n",
       "          [149, 164, 131],\n",
       "          [141, 157, 120],\n",
       "          ...,\n",
       "          [185, 217, 255],\n",
       "          [182, 209, 255],\n",
       "          [201, 221, 254]],\n",
       "\n",
       "         [[153, 168, 137],\n",
       "          [111, 129,  87],\n",
       "          [112, 134,  85],\n",
       "          ...,\n",
       "          [165, 209, 254],\n",
       "          [148, 188, 249],\n",
       "          [182, 207, 248]],\n",
       "\n",
       "         [[157, 171, 146],\n",
       "          [114, 135,  96],\n",
       "          [130, 153, 107],\n",
       "          ...,\n",
       "          [165, 209, 255],\n",
       "          [143, 184, 228],\n",
       "          [175, 204, 234]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[128, 141,  95],\n",
       "          [ 81,  99,  39],\n",
       "          [100, 120,  51],\n",
       "          ...,\n",
       "          [118, 131,  49],\n",
       "          [108, 120,  44],\n",
       "          [155, 164, 111]],\n",
       "\n",
       "         [[135, 147, 101],\n",
       "          [ 93, 109,  46],\n",
       "          [105, 122,  51],\n",
       "          ...,\n",
       "          [171, 185,  98],\n",
       "          [156, 170,  91],\n",
       "          [184, 195, 137]],\n",
       "\n",
       "         [[174, 181, 148],\n",
       "          [144, 153, 108],\n",
       "          [145, 155, 103],\n",
       "          ...,\n",
       "          [216, 227, 161],\n",
       "          [206, 217, 159],\n",
       "          [219, 226, 185]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
